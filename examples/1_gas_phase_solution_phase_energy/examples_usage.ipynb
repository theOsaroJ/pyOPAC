{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# pyOPAC Usage Examples: Gas Phase and Solution Phase Energy Prediction\n",
        "\n",
        "This notebook demonstrates how to use the pyOPAC package to predict **gas phase energy**, **solution phase energy**, **dipole moments**, and **solvation free energy** from molecular structures using SOAP descriptors.\n",
        "\n",
        "## Overview\n",
        "\n",
        "This example is based on the `examples/1_gas_phase_solution_phase_energy` dataset, where we predict:\n",
        "- **Gas_Phase_Energy** (eV): Energy of molecule in gas phase\n",
        "- **Dipole_X, Dipole_Y, Dipole_Z** (Debye): Dipole moment components (vector property)\n",
        "- **Solution_Phase_Energy** (eV): Energy of molecule in solution\n",
        "- **Solvation_Free_Energy** (eV): Free energy of solvation\n",
        "\n",
        "pyOPAC provides:\n",
        "- **SOAP Descriptors**: Size-invariant, rotationally equivariant molecular descriptors (perfect for dipole moments!)\n",
        "- **Property Prediction**: Train neural network models to predict molecular properties\n",
        "- **Multi-target Regression**: Predict multiple properties simultaneously\n",
        "- **Easy-to-use API**: Simple Python interface for all operations\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation and Import\n",
        "\n",
        "First, make sure the package is installed:\n",
        "```bash\n",
        "pip install -e .\n",
        "```\n",
        "\n",
        "Then import the necessary modules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///C:/Users/james/OneDrive/Documents/Research%20ND-%20First%20Year/My%20Publications/pyOPAC/pyOPAC/New%20Work\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Checking if build backend supports build_editable: started\n",
            "  Checking if build backend supports build_editable: finished with status 'done'\n",
            "  Getting requirements to build editable: started\n",
            "  Getting requirements to build editable: finished with status 'done'\n",
            "  Preparing editable metadata (pyproject.toml): started\n",
            "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyOPAC==0.1.0) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyOPAC==0.1.0) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyOPAC==0.1.0) (1.5.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyOPAC==0.1.0) (2.4.1)\n",
            "Requirement already satisfied: ase>=3.21.0 in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyOPAC==0.1.0) (3.23.0)\n",
            "Collecting dscribe>=2.0.0 (from pyOPAC==0.1.0)\n",
            "  Downloading dscribe-2.1.2-cp312-cp312-win_amd64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyOPAC==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ase>=3.21.0->pyOPAC==0.1.0) (3.9.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dscribe>=2.0.0->pyOPAC==0.1.0) (1.4.2)\n",
            "Collecting sparse (from dscribe>=2.0.0->pyOPAC==0.1.0)\n",
            "  Using cached sparse-0.17.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\james\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.2.0->pyOPAC==0.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2.0->pyOPAC==0.1.0) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2.0->pyOPAC==0.1.0) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=0.24.0->pyOPAC==0.1.0) (3.5.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->pyOPAC==0.1.0) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->pyOPAC==0.1.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->pyOPAC==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->pyOPAC==0.1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->pyOPAC==0.1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->pyOPAC==0.1.0) (2024.9.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->pyOPAC==0.1.0) (74.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.4->ase>=3.21.0->pyOPAC==0.1.0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.4->ase>=3.21.0->pyOPAC==0.1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.4->ase>=3.21.0->pyOPAC==0.1.0) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.4->ase>=3.21.0->pyOPAC==0.1.0) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\james\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib>=3.3.4->ase>=3.21.0->pyOPAC==0.1.0) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.4->ase>=3.21.0->pyOPAC==0.1.0) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.4->ase>=3.21.0->pyOPAC==0.1.0) (3.1.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\james\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->pyOPAC==0.1.0) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.8.0->pyOPAC==0.1.0) (2.1.5)\n",
            "Collecting numba>=0.49 (from sparse->dscribe>=2.0.0->pyOPAC==0.1.0)\n",
            "  Downloading numba-0.63.1-cp312-cp312-win_amd64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\james\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.8.0->pyOPAC==0.1.0) (1.3.0)\n",
            "Collecting llvmlite<0.47,>=0.46.0dev0 (from numba>=0.49->sparse->dscribe>=2.0.0->pyOPAC==0.1.0)\n",
            "  Downloading llvmlite-0.46.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
            "Downloading dscribe-2.1.2-cp312-cp312-win_amd64.whl (591 kB)\n",
            "   ---------------------------------------- 0.0/591.2 kB ? eta -:--:--\n",
            "   -- ------------------------------------ 41.0/591.2 kB 991.0 kB/s eta 0:00:01\n",
            "   ---------------------------------------  583.7/591.2 kB 7.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 591.2/591.2 kB 7.4 MB/s eta 0:00:00\n",
            "Using cached sparse-0.17.0-py2.py3-none-any.whl (259 kB)\n",
            "Downloading numba-0.63.1-cp312-cp312-win_amd64.whl (2.8 MB)\n",
            "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
            "   -------------------------- ------------- 1.8/2.8 MB 38.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.8/2.8 MB 44.2 MB/s eta 0:00:00\n",
            "Downloading llvmlite-0.46.0-cp312-cp312-win_amd64.whl (38.1 MB)\n",
            "   ---------------------------------------- 0.0/38.1 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 3.6/38.1 MB 78.1 MB/s eta 0:00:01\n",
            "   ------- -------------------------------- 7.4/38.1 MB 79.6 MB/s eta 0:00:01\n",
            "   ----------- ---------------------------- 11.0/38.1 MB 81.8 MB/s eta 0:00:01\n",
            "   -------------- ------------------------- 14.0/38.1 MB 72.6 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 18.3/38.1 MB 81.8 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 21.9/38.1 MB 81.8 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 25.2/38.1 MB 81.8 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 25.2/38.1 MB 81.8 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 28.8/38.1 MB 59.5 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 32.3/38.1 MB 59.5 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 34.9/38.1 MB 54.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  38.1/38.1 MB 73.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------  38.1/38.1 MB 73.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 38.1/38.1 MB 54.7 MB/s eta 0:00:00\n",
            "Building wheels for collected packages: pyOPAC\n",
            "  Building editable for pyOPAC (pyproject.toml): started\n",
            "  Building editable for pyOPAC (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for pyOPAC: filename=pyopac-0.1.0-0.editable-py3-none-any.whl size=6705 sha256=a849699f27b1197413bc71b23fc7507eb43e762183e13181d2e655f935f7d266\n",
            "  Stored in directory: C:\\Users\\james\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-9sr_h952\\wheels\\2a\\9a\\8d\\0adeeb2eb012ff9d80c6de8760b4c35cc8dd6abde6a0f73639\n",
            "Successfully built pyOPAC\n",
            "Installing collected packages: llvmlite, numba, sparse, dscribe, pyOPAC\n",
            "Successfully installed dscribe-2.1.2 llvmlite-0.46.0 numba-0.63.1 pyOPAC-0.1.0 sparse-0.17.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The scripts pyopac-active-learning.exe, pyopac-compute-descriptors.exe, pyopac-generate.exe, pyopac-modify-xyz.exe, pyopac-predict.exe, pyopac-preprocess.exe and pyopac-train.exe are installed in 'c:\\Users\\james\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pyOPAC version: 0.1.0\n",
            "Authors: Etinosa Osaro and Yamil Colon\n",
            "\n",
            "✓ Package imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import pyOPAC package\n",
        "import opac\n",
        "\n",
        "# Print package information\n",
        "print(f\"pyOPAC version: {opac.__version__}\")\n",
        "print(f\"Authors: {opac.__author__}\")\n",
        "\n",
        "# Import main functions\n",
        "from opac import compute_descriptors, get_all_species, read_xyz_files, MoleculeDataset\n",
        "from opac.models.trainer import train_model, PropertyPredictor, evaluate_model\n",
        "from opac.utils.logger import get_logger\n",
        "\n",
        "# Import other necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "print(\"\\n✓ Package imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Reading Molecular Data\n",
        "\n",
        "We'll use the example data from `examples/1_gas_phase_solution_phase_energy`:\n",
        "- **Training data**: `train_example.xyz` - Contains multiple molecules for training\n",
        "- **Test data**: `test_example.xyz` - Contains molecules for testing\n",
        "- **Target properties**: Gas phase energy, dipole moments, solution phase energy, solvation free energy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: ..\\examples\\1_gas_phase_solution_phase_energy\\train.xyz not found. Creating example molecules for demonstration.\n",
            "Created 2 example molecules for demonstration\n",
            "\n",
            "Total molecules: 2\n"
          ]
        }
      ],
      "source": [
        "# Read molecules from the example XYZ file\n",
        "# Note: Adjust the path to match your actual data location\n",
        "example_data_dir = Path(\"../examples/1_gas_phase_solution_phase_energy\")\n",
        "\n",
        "# Check if example files exist, otherwise use relative paths\n",
        "if not example_data_dir.exists():\n",
        "    # Try alternative path\n",
        "    example_data_dir = Path(\"examples/1_gas_phase_solution_phase_energy\")\n",
        "\n",
        "if example_data_dir.exists():\n",
        "    train_xyz_file = example_data_dir / \"train.xyz\"\n",
        "    \n",
        "    if train_xyz_file.exists():\n",
        "        # Read molecules from XYZ file\n",
        "        molecules = read_xyz_files(str(example_data_dir))\n",
        "        print(f\"✓ Read {len(molecules)} molecules from {example_data_dir}\")\n",
        "        print(f\"\\nFirst molecule:\")\n",
        "        print(f\"  Formula: {molecules[0].get_chemical_formula()}\")\n",
        "        print(f\"  Number of atoms: {len(molecules[0])}\")\n",
        "        print(f\"  Species: {molecules[0].get_chemical_symbols()}\")\n",
        "    else:\n",
        "        print(f\"Note: {train_xyz_file} not found. Creating example molecules for demonstration.\")\n",
        "        from ase import Atoms\n",
        "        molecules = [\n",
        "            Atoms('H2O', positions=[[0, 0, 0], [0.96, 0, 0], [0, 0.96, 0]]),\n",
        "            Atoms('CO2', positions=[[0, 0, 0], [1.16, 0, 0], [-1.16, 0, 0]]),\n",
        "        ]\n",
        "        print(f\"Created {len(molecules)} example molecules for demonstration\")\n",
        "else:\n",
        "    print(f\"Note: {example_data_dir} not found. Creating example molecules for demonstration.\")\n",
        "    from ase import Atoms\n",
        "    molecules = [\n",
        "        Atoms('H2O', positions=[[0, 0, 0], [0.96, 0, 0], [0, 0.96, 0]]),\n",
        "        Atoms('CO2', positions=[[0, 0, 0], [1.16, 0, 0], [-1.16, 0, 0]]),\n",
        "    ]\n",
        "    print(f\"Created {len(molecules)} example molecules for demonstration\")\n",
        "\n",
        "print(f\"\\nTotal molecules: {len(molecules)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Computing SOAP Descriptors\n",
        "\n",
        "SOAP (Smooth Overlap of Atomic Positions) descriptors are ideal for predicting:\n",
        "- **Gas/Solution Phase Energies**: Capture 3D molecular structure\n",
        "- **Dipole Moments**: Rotationally equivariant - perfect for vector properties!\n",
        "- **Solvation Free Energy**: Size-invariant descriptors work for molecules of any size\n",
        "\n",
        "### Key Features:\n",
        "- **Size-invariant**: Fixed-size descriptors regardless of molecule size\n",
        "- **Rotationally equivariant**: Transform predictably with rotations (critical for dipole moments)\n",
        "- **Translationally invariant**: Same descriptor regardless of position\n",
        "- **3D structure-aware**: Based on actual atomic positions\n",
        "\n",
        "### 3.1 Getting All Species\n",
        "\n",
        "For size-invariant descriptors, we need to use a fixed species list across all molecules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all unique species from your dataset\n",
        "# This ensures fixed-size descriptors across all molecules\n",
        "all_species = get_all_species(molecules)\n",
        "\n",
        "print(f\"All species found in dataset: {all_species}\")\n",
        "print(f\"Number of unique species: {len(all_species)}\")\n",
        "\n",
        "# Save species list for later use (important for test set!)\n",
        "species_list_file = \"species_list.json\"\n",
        "with open(species_list_file, 'w') as f:\n",
        "    json.dump(all_species, f)\n",
        "print(f\"\\n✓ Species list saved to {species_list_file}\")\n",
        "print(\"  This list will be used for all molecules (training and test) to ensure size-invariance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Computing SOAP Descriptors\n",
        "\n",
        "Now let's compute SOAP descriptors for a molecule:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute SOAP descriptors for the first molecule as an example\n",
        "# Use fixed species list for size-invariant descriptors\n",
        "desc = compute_descriptors(\n",
        "    molecules[0], \n",
        "    species=all_species,  # Fixed species list ensures size-invariance\n",
        "    rcut=6.0,             # Cutoff radius in Angstrom (larger = more neighbors)\n",
        "    nmax=6,               # Number of radial basis functions (higher = more radial detail)\n",
        "    lmax=4,               # Maximum angular momentum (higher = more angular detail, better for dipole moments)\n",
        "    sigma=0.3             # Width of Gaussian smearing (controls locality)\n",
        ")\n",
        "\n",
        "print(f\"Example molecule: {molecules[0].get_chemical_formula()}\")\n",
        "print(f\"Number of atoms: {len(molecules[0])}\")\n",
        "print(f\"Number of SOAP descriptors: {len(desc)}\")\n",
        "print(f\"\\nDescriptor keys (first 10): {list(desc.keys())[:10]}\")\n",
        "print(f\"Descriptor values (first 10): {[f'{v:.6f}' for v in list(desc.values())[:10]]}\")\n",
        "\n",
        "# Convert to list for easier handling\n",
        "descriptor_values = list(desc.values())\n",
        "print(f\"\\nDescriptor shape: {len(descriptor_values)} features\")\n",
        "print(\"\\n✓ SOAP descriptors computed successfully!\")\n",
        "print(\"\\nNote: Higher lmax values are better for vector properties like dipole moments!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Computing Descriptors for All Molecules\n",
        "\n",
        "Now let's compute SOAP descriptors for all training molecules. This ensures all molecules have the same descriptor size (size-invariant):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display information about all molecules\n",
        "print(f\"Processing {len(molecules)} molecules...\")\n",
        "print(\"\\nMolecule information:\")\n",
        "for i, mol in enumerate(molecules[:5]):  # Show first 5\n",
        "    print(f\"  Molecule {i+1}: {mol.get_chemical_formula()} ({len(mol)} atoms)\")\n",
        "if len(molecules) > 5:\n",
        "    print(f\"  ... and {len(molecules) - 5} more molecules\")\n",
        "\n",
        "print(f\"\\nAll species in dataset: {all_species}\")\n",
        "print(f\"Using fixed species list for size-invariant descriptors: {all_species}\")\n",
        "print(\"\\n✓ Molecules prepared for descriptor computation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute SOAP descriptors for all molecules with fixed species list\n",
        "# This ensures size-invariant descriptors (same size for all molecules)\n",
        "print(\"Computing SOAP descriptors for all molecules...\")\n",
        "descriptors_list = []\n",
        "\n",
        "for idx, atoms in enumerate(molecules):\n",
        "    try:\n",
        "        # Use fixed species list to ensure size-invariance\n",
        "        desc = compute_descriptors(atoms, species=all_species)\n",
        "        desc['mol_id'] = idx\n",
        "        descriptors_list.append(desc)\n",
        "        \n",
        "        if (idx + 1) % 10 == 0 or idx < 3:  # Show progress every 10 molecules and first 3\n",
        "            print(f\"  Molecule {idx+1}/{len(molecules)} ({atoms.get_chemical_formula()}): \"\n",
        "                  f\"{len(desc)-1} descriptors\")  # -1 for mol_id\n",
        "    except Exception as e:\n",
        "        print(f\"  Error processing molecule {idx+1}: {e}\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_descriptors = pd.DataFrame(descriptors_list)\n",
        "\n",
        "# Reorder columns to put mol_id first\n",
        "cols = ['mol_id'] + [col for col in df_descriptors.columns if col != 'mol_id']\n",
        "df_descriptors = df_descriptors[cols]\n",
        "\n",
        "print(f\"\\n✓ Descriptors computed successfully!\")\n",
        "print(f\"DataFrame shape: {df_descriptors.shape} (molecules × features)\")\n",
        "print(f\"All molecules have the same descriptor size: {len(df_descriptors.columns) - 1} SOAP features\")\n",
        "\n",
        "# Save descriptors to CSV\n",
        "descriptors_file = \"descriptors.csv\"\n",
        "df_descriptors.to_csv(descriptors_file, index=False)\n",
        "print(f\"\\n✓ Descriptors saved to {descriptors_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Loading Target Properties from DFT Calculations\n",
        "\n",
        "The target properties come from DFT calculations (see `examples/1_gas_phase_solution_phase_energy/dft_calculation.py`).\n",
        "\n",
        "The `results_train.dat` file contains:\n",
        "- **Gas_Phase_Energy** (eV): Energy in gas phase (column 1)\n",
        "- **Solution_Phase_Energy** (eV): Energy in solution phase (column 2)\n",
        "- **Solvation_Free_Energy** (eV): Calculated as Solution_Phase_Energy - Gas_Phase_Energy\n",
        "\n",
        "Note: Dipole moments (Dipole_X, Dipole_Y, Dipole_Z) are not included in this file format. They can be added separately if available.\n",
        "\n",
        "Let's load the target properties:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load target properties from results_train.dat\n",
        "# This file contains DFT calculation results (gas phase and solution phase energies)\n",
        "# Format: Gas_Phase_Energy(eV),Solution_Phase_Energy(eV)\n",
        "# (Dipole moments may be in a separate file or computed separately)\n",
        "\n",
        "results_file = Path(\"results_train.dat\")\n",
        "\n",
        "if results_file.exists():\n",
        "    # Read the results file (no header, comma-separated)\n",
        "    df_results = pd.read_csv(results_file, header=None, names=['Gas_Phase_Energy', 'Solution_Phase_Energy'])\n",
        "    \n",
        "    print(f\"✓ Loaded target properties from {results_file}\")\n",
        "    print(f\"Number of molecules: {len(df_results)}\")\n",
        "    print(f\"Properties: {list(df_results.columns)}\")\n",
        "    \n",
        "    # Calculate solvation free energy (difference between solution and gas phase)\n",
        "    df_results['Solvation_Free_Energy'] = df_results['Solution_Phase_Energy'] - df_results['Gas_Phase_Energy']\n",
        "    \n",
        "    # Add molecule IDs\n",
        "    df_results['mol_id'] = range(len(df_results))\n",
        "    \n",
        "    # Reorder columns\n",
        "    df_targets = df_results[['mol_id', 'Gas_Phase_Energy', 'Solution_Phase_Energy', 'Solvation_Free_Energy']]\n",
        "    \n",
        "    print(f\"\\nEnergy statistics:\")\n",
        "    print(f\"  Gas Phase Energy range: [{df_targets['Gas_Phase_Energy'].min():.2f}, {df_targets['Gas_Phase_Energy'].max():.2f}] eV\")\n",
        "    print(f\"  Solution Phase Energy range: [{df_targets['Solution_Phase_Energy'].min():.2f}, {df_targets['Solution_Phase_Energy'].max():.2f}] eV\")\n",
        "    print(f\"  Solvation Free Energy range: [{df_targets['Solvation_Free_Energy'].min():.4f}, {df_targets['Solvation_Free_Energy'].max():.4f}] eV\")\n",
        "    \n",
        "    print(f\"\\nNote: Dipole moments (Dipole_X, Dipole_Y, Dipole_Z) are not in this file.\")\n",
        "    print(f\"      They can be computed separately or added if available in another file.\")\n",
        "    \n",
        "else:\n",
        "    print(f\"Note: {results_file} not found. Creating example target properties.\")\n",
        "    print(\"In practice, load from results_train.dat (from DFT calculations)\")\n",
        "    \n",
        "    targets_list = []\n",
        "    for idx in range(len(molecules)):\n",
        "        targets_list.append({\n",
        "            'mol_id': idx,\n",
        "            'Gas_Phase_Energy': -400.0 - np.random.rand() * 100,  # Example energy in eV\n",
        "            'Solution_Phase_Energy': -400.0 - np.random.rand() * 100 - 0.1,  # Example soln energy\n",
        "            'Solvation_Free_Energy': -0.1 - np.random.rand() * 0.1,          # Example solvation energy\n",
        "        })\n",
        "    \n",
        "    df_targets = pd.DataFrame(targets_list)\n",
        "\n",
        "print(f\"\\nTarget properties shape: {df_targets.shape}\")\n",
        "print(f\"Target columns: {[col for col in df_targets.columns if col != 'mol_id']}\")\n",
        "print(\"\\nFirst few target properties:\")\n",
        "print(df_targets.head())\n",
        "\n",
        "# Save targets to CSV\n",
        "targets_output_file = \"targets.csv\"\n",
        "df_targets.to_csv(targets_output_file, index=False)\n",
        "print(f\"\\n✓ Target properties saved to {targets_output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Preparing Data for Training\n",
        "\n",
        "Now let's prepare the data for training a multi-target regression model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Merge descriptors and targets on mol_id\n",
        "df = pd.merge(df_descriptors, df_targets, on='mol_id', how='inner')\n",
        "print(f\"✓ Merged descriptors and targets: {df.shape}\")\n",
        "\n",
        "# Identify descriptor and target columns\n",
        "descriptor_columns = [col for col in df_descriptors.columns if col != 'mol_id']\n",
        "target_columns = [col for col in df_targets.columns if col != 'mol_id']\n",
        "\n",
        "print(f\"\\nDescriptor features: {len(descriptor_columns)}\")\n",
        "print(f\"Target properties: {target_columns}\")\n",
        "\n",
        "# Split into train/test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(train_df)} molecules\")\n",
        "print(f\"Test set size: {len(test_df)} molecules\")\n",
        "\n",
        "# Prepare data for MoleculeDataset\n",
        "train_descriptors = train_df[descriptor_columns].to_dict('records')\n",
        "train_targets = train_df[target_columns].to_dict('records')\n",
        "test_descriptors = test_df[descriptor_columns].to_dict('records')\n",
        "test_targets = test_df[target_columns].to_dict('records')\n",
        "\n",
        "print(\"\\n✓ Data prepared for training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training the Model\n",
        "\n",
        "Now let's train a neural network to predict all properties simultaneously (multi-target regression):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "train_dataset = MoleculeDataset(train_descriptors, train_targets)\n",
        "test_dataset = MoleculeDataset(test_descriptors, test_targets)\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)} molecules\")\n",
        "print(f\"Input dimension (SOAP descriptors): {train_dataset.input_dim}\")\n",
        "print(f\"Output dimension (target properties): {train_dataset.output_dim}\")\n",
        "print(f\"Target properties: {target_columns}\")\n",
        "\n",
        "# Train model\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training multi-target regression model...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model = train_model(\n",
        "    train_dataset,\n",
        "    input_dim=train_dataset.input_dim,\n",
        "    output_dim=train_dataset.output_dim,\n",
        "    epochs=100,          # Number of training epochs\n",
        "    batch_size=32,       # Batch size\n",
        "    learning_rate=1e-3,  # Learning rate\n",
        "    hidden_dim=128,      # Hidden layer dimension\n",
        "    weight_decay=1e-4     # L2 regularization\n",
        ")\n",
        "\n",
        "# Save model\n",
        "import torch\n",
        "model_file = \"saved_models/trained_model.pth\"\n",
        "Path(\"saved_models\").mkdir(exist_ok=True)\n",
        "torch.save(model.state_dict(), model_file)\n",
        "print(f\"\\n✓ Model trained and saved to {model_file}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluating Model Performance\n",
        "\n",
        "Let's evaluate the model on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model on test set\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Evaluating model on test set...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "avg_loss, per_target_metrics = evaluate_model(model, test_dataset, batch_size=32)\n",
        "\n",
        "print(f\"\\nOverall Test Loss (MSE): {avg_loss:.6f}\")\n",
        "print(\"\\nPer-target performance metrics:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for metrics in per_target_metrics:\n",
        "    target_idx = metrics['target_index']\n",
        "    target_name = target_columns[target_idx]\n",
        "    print(f\"\\n{target_name}:\")\n",
        "    print(f\"  MSE: {metrics['MSE']:.6f}\")\n",
        "    print(f\"  MAE: {metrics['MAE']:.6f}\")\n",
        "    print(f\"  R² Score: {metrics['R2_Score']:.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✓ Model evaluation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Making Predictions on New Molecules\n",
        "\n",
        "Now let's see how to make predictions on new molecules (test set):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on test molecules\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "model.eval()\n",
        "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
        "\n",
        "predictions = []\n",
        "targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        inputs = batch['descriptors']\n",
        "        outputs = model(inputs)\n",
        "        predictions.append(outputs.numpy())\n",
        "        targets.append(batch['targets'].numpy())\n",
        "\n",
        "predictions = np.concatenate(predictions, axis=0)\n",
        "targets = np.concatenate(targets, axis=0)\n",
        "\n",
        "print(f\"Predictions shape: {predictions.shape} (molecules × properties)\")\n",
        "print(f\"Targets shape: {targets.shape}\")\n",
        "\n",
        "# Create predictions DataFrame\n",
        "predictions_df = pd.DataFrame(predictions, columns=target_columns)\n",
        "predictions_df.insert(0, 'mol_id', test_df['mol_id'].values)\n",
        "\n",
        "# Save predictions\n",
        "predictions_file = \"predictions.csv\"\n",
        "predictions_df.to_csv(predictions_file, index=False)\n",
        "print(f\"\\n✓ Predictions saved to {predictions_file}\")\n",
        "\n",
        "# Show sample predictions vs actual\n",
        "print(\"\\nSample predictions vs actual values:\")\n",
        "comparison_df = pd.DataFrame({\n",
        "    'mol_id': test_df['mol_id'].values\n",
        "})\n",
        "for i, target_name in enumerate(target_columns):\n",
        "    comparison_df[f'{target_name}_actual'] = targets[:, i]\n",
        "    comparison_df[f'{target_name}_predicted'] = predictions[:, i]\n",
        "    comparison_df[f'{target_name}_error'] = np.abs(targets[:, i] - predictions[:, i])\n",
        "\n",
        "print(comparison_df.head(10))\n",
        "print(\"\\n✓ Predictions complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Complete Workflow: Step-by-Step with Python and Bash\n",
        "\n",
        "This section shows the complete workflow using both Python API and command-line tools.\n",
        "\n",
        "### 10.1 Step 1: Modify Training XYZ Files\n",
        "\n",
        "**Python Code:**\n",
        "```python\n",
        "import os\n",
        "import shutil\n",
        "from creating_the_xyz.modify import modify_xyz_file\n",
        "\n",
        "# Define file paths\n",
        "input_xyz = \"train_example.xyz\"\n",
        "modified_xyz = \"train.xyz\"\n",
        "dest_dir = os.path.join(\"dataset\", \"training_xyz_files\")\n",
        "dest_file = os.path.join(dest_dir, \"train.xyz\")\n",
        "\n",
        "# Modify the training XYZ file\n",
        "modify_xyz_file(input_xyz, modified_xyz)\n",
        "print(f\"Modified file created: {modified_xyz}\")\n",
        "\n",
        "# Ensure the destination directory exists and copy the file there\n",
        "os.makedirs(dest_dir, exist_ok=True)\n",
        "shutil.copy(modified_xyz, dest_file)\n",
        "print(f\"Modified training file copied to {dest_file}\")\n",
        "```\n",
        "\n",
        "**Bash Command:**\n",
        "```bash\n",
        "# Modify XYZ files to add molecule IDs\n",
        "pyopac-modify-xyz train_example.xyz train.xyz\n",
        "\n",
        "# Create directory and copy\n",
        "mkdir -p dataset/training_xyz_files\n",
        "cp train.xyz dataset/training_xyz_files/\n",
        "```\n",
        "\n",
        "### 10.2 Step 2: Preprocess Training Data and Compute SOAP Descriptors\n",
        "\n",
        "**Python Code:**\n",
        "```python\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from opac.data.loader import read_xyz_files\n",
        "from opac.data.descriptors import compute_descriptors, get_all_species\n",
        "\n",
        "# Directory with training XYZ files\n",
        "input_dir = os.path.join(\"dataset\", \"training_xyz_files\")\n",
        "\n",
        "# Read molecules from the XYZ files\n",
        "molecules = read_xyz_files(input_dir)\n",
        "print(f\"Read {len(molecules)} molecules from {input_dir}\")\n",
        "\n",
        "# Get all unique species for fixed-size descriptors (size-invariant)\n",
        "all_species = get_all_species(molecules)\n",
        "print(f\"All species found: {all_species}\")\n",
        "\n",
        "# Save species list for later use (important for test set!)\n",
        "species_list_file = \"species_list.json\"\n",
        "with open(species_list_file, 'w') as f:\n",
        "    json.dump(all_species, f)\n",
        "print(f\"Species list saved to {species_list_file}\")\n",
        "\n",
        "# Compute descriptors for each molecule\n",
        "descriptors_list = []\n",
        "for idx, atoms in enumerate(molecules):\n",
        "    try:\n",
        "        # Use fixed species list for size-invariant descriptors\n",
        "        desc = compute_descriptors(atoms, species=all_species)\n",
        "        desc['mol_id'] = idx  # assign molecule ID\n",
        "        descriptors_list.append(desc)\n",
        "        if (idx + 1) % 100 == 0:\n",
        "            print(f\"Processed {idx + 1}/{len(molecules)} molecules\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to compute descriptors for molecule {idx}: {e}\")\n",
        "\n",
        "# Create DataFrame and reorder columns so that 'mol_id' comes first\n",
        "df_descriptors = pd.DataFrame(descriptors_list)\n",
        "cols = ['mol_id'] + [col for col in df_descriptors.columns if col != 'mol_id']\n",
        "df_descriptors = df_descriptors[cols]\n",
        "\n",
        "# Save descriptors to CSV\n",
        "output_csv = os.path.join(\"dataset\", \"descriptors.csv\")\n",
        "df_descriptors.to_csv(output_csv, index=False)\n",
        "print(f\"Descriptors saved to {output_csv}\")\n",
        "print(f\"Descriptor shape: {df_descriptors.shape} (molecules × features)\")\n",
        "```\n",
        "\n",
        "**Bash Command:**\n",
        "```bash\n",
        "# Preprocess data and compute descriptors\n",
        "pyopac-preprocess \\\n",
        "  --input-dir dataset/training_xyz_files/ \\\n",
        "  --targets-file dataset/targets.csv \\\n",
        "  --output-descriptors dataset/descriptors.csv\n",
        "\n",
        "# Or compute descriptors only (without targets)\n",
        "pyopac-compute-descriptors \\\n",
        "  --input-dir dataset/training_xyz_files/ \\\n",
        "  --output-descriptors dataset/descriptors.csv\n",
        "```\n",
        "\n",
        "### 10.3 Step 3: Train Model (Standard)\n",
        "\n",
        "**Python Code:**\n",
        "```python\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from opac.data.dataset import MoleculeDataset\n",
        "from opac.models.trainer import train_model, evaluate_model\n",
        "\n",
        "# Load descriptors and targets\n",
        "df_descriptors = pd.read_csv(os.path.join(\"dataset\", \"descriptors.csv\"))\n",
        "df_targets = pd.read_csv(os.path.join(\"dataset\", \"targets.csv\"))\n",
        "\n",
        "# Ensure 'mol_id' is numeric and merge on 'mol_id'\n",
        "df_descriptors['mol_id'] = df_descriptors['mol_id'].astype(int)\n",
        "df_targets['mol_id'] = df_targets['mol_id'].astype(int)\n",
        "df = pd.merge(df_descriptors, df_targets, on='mol_id')\n",
        "\n",
        "# Identify descriptor and target columns\n",
        "descriptor_cols = [col for col in df_descriptors.columns if col != 'mol_id']\n",
        "target_cols = [col for col in df_targets.columns if col != 'mol_id']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_descriptors = train_df[descriptor_cols].to_dict('records')\n",
        "train_targets = train_df[target_cols].to_dict('records')\n",
        "test_descriptors = test_df[descriptor_cols].to_dict('records')\n",
        "test_targets = test_df[target_cols].to_dict('records')\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = MoleculeDataset(train_descriptors, train_targets)\n",
        "test_dataset = MoleculeDataset(test_descriptors, test_targets)\n",
        "\n",
        "# Get dimensions for model input/output\n",
        "input_dim = train_dataset.input_dim\n",
        "output_dim = train_dataset.output_dim\n",
        "\n",
        "# Train the model\n",
        "model = train_model(\n",
        "    dataset=train_dataset,\n",
        "    input_dim=input_dim,\n",
        "    output_dim=output_dim,\n",
        "    epochs=200,\n",
        "    batch_size=64,\n",
        "    learning_rate=0.001,\n",
        "    hidden_dim=512,\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "# Save the trained model and parameters\n",
        "model_path = os.path.join(\"saved_models\", \"trained_model.pth\")\n",
        "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")\n",
        "\n",
        "model_params = {\"input_dim\": input_dim, \"hidden_dim\": 512, \"output_dim\": output_dim}\n",
        "params_path = model_path + \".params.json\"\n",
        "with open(params_path, \"w\") as f:\n",
        "    json.dump(model_params, f)\n",
        "print(f\"Model parameters saved to {params_path}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, per_target_metrics = evaluate_model(model, test_dataset, batch_size=64)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "for metric in per_target_metrics:\n",
        "    print(metric)\n",
        "```\n",
        "\n",
        "**Bash Command:**\n",
        "```bash\n",
        "# Train model with all options\n",
        "pyopac-train \\\n",
        "  --descriptors-file dataset/descriptors.csv \\\n",
        "  --targets-file dataset/targets.csv \\\n",
        "  --model-output saved_models/trained_model.pth \\\n",
        "  --epochs 200 \\\n",
        "  --batch-size 64 \\\n",
        "  --learning-rate 0.001 \\\n",
        "  --hidden-dim 512 \\\n",
        "  --weight-decay 1e-4 \\\n",
        "  --test-size 0.2\n",
        "```\n",
        "\n",
        "### 10.4 Step 3b: Train Model with Hyperparameter Tuning\n",
        "\n",
        "**Python Code:**\n",
        "```python\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "import itertools\n",
        "from opac.data.dataset import MoleculeDataset\n",
        "from opac.models.trainer import train_model, evaluate_model\n",
        "\n",
        "# Load descriptors and targets\n",
        "df_descriptors = pd.read_csv(os.path.join(\"dataset\", \"descriptors.csv\"))\n",
        "df_targets = pd.read_csv(os.path.join(\"dataset\", \"targets.csv\"))\n",
        "\n",
        "# Ensure 'mol_id' is numeric and merge on 'mol_id'\n",
        "df_descriptors['mol_id'] = df_descriptors['mol_id'].astype(int)\n",
        "df_targets['mol_id'] = df_targets['mol_id'].astype(int)\n",
        "df = pd.merge(df_descriptors, df_targets, on='mol_id')\n",
        "\n",
        "# Identify descriptor and target columns\n",
        "descriptor_cols = [col for col in df_descriptors.columns if col != 'mol_id']\n",
        "target_cols = [col for col in df_targets.columns if col != 'mol_id']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_descriptors = train_df[descriptor_cols].to_dict('records')\n",
        "train_targets = train_df[target_cols].to_dict('records')\n",
        "test_descriptors = test_df[descriptor_cols].to_dict('records')\n",
        "test_targets = test_df[target_cols].to_dict('records')\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = MoleculeDataset(train_descriptors, train_targets)\n",
        "test_dataset = MoleculeDataset(test_descriptors, test_targets)\n",
        "\n",
        "# Get dimensions for model input/output\n",
        "input_dim = train_dataset.input_dim\n",
        "output_dim = train_dataset.output_dim\n",
        "\n",
        "# Example hyperparameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.0001, 0.001, 0.01],\n",
        "    'hidden_dim': [128, 256, 512],\n",
        "    'weight_decay': [0.0, 1e-4, 1e-3]\n",
        "}\n",
        "\n",
        "# Hyperparameter tuning via grid search\n",
        "best_loss = float('inf')\n",
        "best_params = None\n",
        "best_model_state = None\n",
        "\n",
        "# Create combinations of hyperparameters\n",
        "for lr, hidden, wd in itertools.product(param_grid['learning_rate'],\n",
        "                                         param_grid['hidden_dim'],\n",
        "                                         param_grid['weight_decay']):\n",
        "    print(f\"Training with lr={lr}, hidden_dim={hidden}, weight_decay={wd}\")\n",
        "\n",
        "    # Train the model using the current hyperparameters\n",
        "    model = train_model(\n",
        "        dataset=train_dataset,\n",
        "        input_dim=input_dim,\n",
        "        output_dim=output_dim,\n",
        "        epochs=100,  # You can adjust the number of epochs\n",
        "        batch_size=64,\n",
        "        learning_rate=lr,\n",
        "        hidden_dim=hidden,\n",
        "        weight_decay=wd\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_loss, _ = evaluate_model(model, test_dataset, batch_size=32)\n",
        "    print(f\"Validation loss: {test_loss:.4f}\")\n",
        "    \n",
        "    # If current configuration is better, store its parameters and model state\n",
        "    if test_loss < best_loss:\n",
        "        best_loss = test_loss\n",
        "        best_params = {\n",
        "            'input_dim': input_dim, \n",
        "            'output_dim': output_dim,\n",
        "            'learning_rate': lr, \n",
        "            'hidden_dim': hidden, \n",
        "            'weight_decay': wd\n",
        "        }\n",
        "        best_model_state = model.state_dict()  # Save the state_dict\n",
        "\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(best_params)\n",
        "print(f\"Best Validation Loss: {best_loss:.4f}\")\n",
        "\n",
        "# Save best model\n",
        "torch.save(best_model_state, \"saved_models/best_trained_model.pth\")\n",
        "with open(\"saved_models/best_trained_model.pth.params.json\", \"w\") as f:\n",
        "    json.dump(best_params, f)\n",
        "```\n",
        "\n",
        "**Bash Command:**\n",
        "```bash\n",
        "# Note: Hyperparameter tuning is typically done via Python scripts\n",
        "# You can create a script and run it:\n",
        "python notebooks/03_Train_Model_with_hyperparameters.py\n",
        "```\n",
        "\n",
        "### 10.5 Step 4: Modify Test XYZ Files\n",
        "\n",
        "**Python Code:**\n",
        "```python\n",
        "import os\n",
        "import shutil\n",
        "from creating_the_xyz.modify import modify_xyz_file\n",
        "\n",
        "# Define file paths for test XYZ\n",
        "input_test_xyz = \"test_example.xyz\"\n",
        "modified_test_xyz = \"test.xyz\"\n",
        "dest_dir = os.path.join(\"dataset\", \"testing_xyz_files\")\n",
        "dest_file = os.path.join(dest_dir, \"test.xyz\")\n",
        "\n",
        "# Modify the test XYZ file\n",
        "modify_xyz_file(input_test_xyz, modified_test_xyz)\n",
        "print(f\"Modified test XYZ file created: {modified_test_xyz}\")\n",
        "\n",
        "# Copy the modified file to the testing directory\n",
        "os.makedirs(dest_dir, exist_ok=True)\n",
        "shutil.copy(modified_test_xyz, dest_file)\n",
        "print(f\"Modified test file copied to {dest_file}\")\n",
        "```\n",
        "\n",
        "**Bash Command:**\n",
        "```bash\n",
        "# Modify test XYZ files\n",
        "pyopac-modify-xyz test_example.xyz test.xyz\n",
        "\n",
        "# Create directory and copy\n",
        "mkdir -p dataset/testing_xyz_files\n",
        "cp test.xyz dataset/testing_xyz_files/\n",
        "```\n",
        "\n",
        "### 10.6 Step 5: Compute Test Descriptors\n",
        "\n",
        "**Python Code:**\n",
        "```python\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from opac.data.loader import read_xyz_files\n",
        "from opac.data.descriptors import compute_descriptors\n",
        "\n",
        "# Load species list from training (CRITICAL for size-invariance!)\n",
        "species_list_file = \"species_list.json\"\n",
        "with open(species_list_file, 'r') as f:\n",
        "    all_species = json.load(f)\n",
        "print(f\"Using fixed species list: {all_species}\")\n",
        "\n",
        "# Directory containing test XYZ files\n",
        "input_dir = os.path.join(\"dataset\", \"testing_xyz_files\")\n",
        "\n",
        "# Read molecules from the test XYZ files\n",
        "molecules = read_xyz_files(input_dir)\n",
        "print(f\"Read {len(molecules)} molecules from {input_dir}\")\n",
        "\n",
        "# Compute descriptors for each test molecule (using same species list!)\n",
        "descriptors_list = []\n",
        "for idx, atoms in enumerate(molecules):\n",
        "    try:\n",
        "        # Use the SAME species list as training for size-invariance\n",
        "        desc = compute_descriptors(atoms, species=all_species)\n",
        "        desc['mol_id'] = idx\n",
        "        descriptors_list.append(desc)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to compute descriptors for molecule {idx}: {e}\")\n",
        "\n",
        "# Create a DataFrame and reorder columns so that 'mol_id' comes first\n",
        "df_new = pd.DataFrame(descriptors_list)\n",
        "cols = ['mol_id'] + [col for col in df_new.columns if col != 'mol_id']\n",
        "df_new = df_new[cols]\n",
        "\n",
        "# Save new descriptors to CSV\n",
        "output_csv = os.path.join(\"dataset\", \"new_descriptors.csv\")\n",
        "df_new.to_csv(output_csv, index=False)\n",
        "print(f\"New descriptors saved to {output_csv}\")\n",
        "```\n",
        "\n",
        "**Bash Command:**\n",
        "```bash\n",
        "# Compute descriptors for test molecules\n",
        "pyopac-compute-descriptors \\\n",
        "  --input-dir dataset/testing_xyz_files/ \\\n",
        "  --output-descriptors dataset/new_descriptors.csv\n",
        "```\n",
        "\n",
        "### 10.7 Step 6: Predict Properties\n",
        "\n",
        "**Python Code:**\n",
        "```python\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "from opac.data.dataset import MoleculeDataset\n",
        "from opac.models.trainer import PropertyPredictor\n",
        "\n",
        "# Load new descriptors\n",
        "df_new = pd.read_csv(os.path.join(\"dataset\", \"new_descriptors.csv\"))\n",
        "descriptor_cols = [col for col in df_new.columns if col != 'mol_id']\n",
        "descriptors = df_new[descriptor_cols].to_dict(\"records\")\n",
        "dataset = MoleculeDataset(descriptors, targets=None)\n",
        "\n",
        "# Load model parameters\n",
        "model_path = os.path.join(\"saved_models\", \"trained_model.pth\")\n",
        "params_path = model_path + \".params.json\"\n",
        "with open(params_path, \"r\") as f:\n",
        "    model_params = json.load(f)\n",
        "input_dim = model_params[\"input_dim\"]\n",
        "hidden_dim = model_params[\"hidden_dim\"]\n",
        "output_dim = model_params[\"output_dim\"]\n",
        "\n",
        "# Initialize and load the model\n",
        "model = PropertyPredictor(input_dim, hidden_dim, output_dim)\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "\n",
        "# Make predictions for each molecule\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for sample in dataset:\n",
        "        # Add batch dimension to the tensor\n",
        "        inputs = sample[\"descriptors\"].unsqueeze(0)\n",
        "        outputs = model(inputs)\n",
        "        predictions.append(outputs.numpy().flatten())\n",
        "\n",
        "# Create a DataFrame with predictions\n",
        "property_names = [f\"Predicted_Property_{i+1}\" for i in range(output_dim)]\n",
        "df_preds = pd.DataFrame(predictions, columns=property_names)\n",
        "df_preds[\"mol_id\"] = df_new[\"mol_id\"]\n",
        "\n",
        "# Save predictions to CSV\n",
        "output_csv = os.path.join(\"dataset\", \"predictions.csv\")\n",
        "df_preds.to_csv(output_csv, index=False)\n",
        "print(f\"Predictions saved to {output_csv}\")\n",
        "```\n",
        "\n",
        "**Bash Command:**\n",
        "```bash\n",
        "# Make predictions using trained model\n",
        "pyopac-predict \\\n",
        "  --model-file saved_models/trained_model.pth \\\n",
        "  --descriptors-file dataset/new_descriptors.csv \\\n",
        "  --predictions-output dataset/predictions.csv\n",
        "```\n",
        "\n",
        "### 10.8 Step 7: Active Learning\n",
        "\n",
        "**Python Code:**\n",
        "```python\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from copy import deepcopy\n",
        "from opac.data.dataset import MoleculeDataset\n",
        "from opac.models.trainer import train_model, evaluate_model\n",
        "from opac.active_learning.predict_with_uncertainty import predict_with_uncertainty\n",
        "from opac.active_learning.uncertainty_sampling import select_most_uncertain_samples\n",
        "from opac.utils.logger import get_logger\n",
        "\n",
        "logger = get_logger(__name__)\n",
        "\n",
        "# Set hyperparameters for active learning\n",
        "initial_train_size = 1000\n",
        "query_size = 5\n",
        "requested_iterations = 2  # maximum number of active learning iterations\n",
        "hidden_dim = 128\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 0.0\n",
        "\n",
        "# File paths\n",
        "descriptors_file = os.path.join(\"dataset\", \"descriptors.csv\")\n",
        "targets_file = os.path.join(\"dataset\", \"targets.csv\")\n",
        "model_output = os.path.join(\"saved_models\", \"al_trained_model.pth\")\n",
        "final_al_training_data = os.path.join(\"dataset\", \"final_al_training_data.csv\")\n",
        "\n",
        "# Load the descriptors and targets from CSV files\n",
        "df_descriptors = pd.read_csv(descriptors_file)\n",
        "df_targets = pd.read_csv(targets_file)\n",
        "\n",
        "# Merge the data on 'mol_id'\n",
        "df = pd.merge(df_descriptors, df_targets, on=\"mol_id\")\n",
        "descriptor_columns = [col for col in df_descriptors.columns if col != \"mol_id\"]\n",
        "target_columns = [col for col in df_targets.columns if col != \"mol_id\"]\n",
        "\n",
        "# Initialize the labeled (training) and unlabeled datasets\n",
        "initial_train_df = df.sample(n=initial_train_size, random_state=42)\n",
        "unlabeled_df = df.drop(initial_train_df.index).reset_index(drop=True)\n",
        "\n",
        "logger.info(f\"Initial training set size: {len(initial_train_df)}\")\n",
        "logger.info(f\"Unlabeled set size: {len(unlabeled_df)}\")\n",
        "\n",
        "# Determine the maximum possible iterations based on available unlabeled data\n",
        "max_possible_iterations = (len(df) - initial_train_size) // query_size\n",
        "iterations = min(requested_iterations, max_possible_iterations)\n",
        "if iterations == 0:\n",
        "    logger.info(\"Not enough data for the specified iterations and query size.\")\n",
        "else:\n",
        "    logger.info(f\"Active learning will run for {iterations} iterations.\")\n",
        "\n",
        "# Active Learning Loop\n",
        "model = None  # No pre-trained model to start with\n",
        "\n",
        "for iteration in range(iterations):\n",
        "    logger.info(f\"--- Active Learning Iteration {iteration + 1}/{iterations} ---\")\n",
        "    \n",
        "    # Create training dataset from the current labeled data\n",
        "    train_descriptors = initial_train_df[descriptor_columns].to_dict(\"records\")\n",
        "    train_targets = initial_train_df[target_columns].to_dict(\"records\")\n",
        "    train_dataset = MoleculeDataset(train_descriptors, train_targets)\n",
        "    \n",
        "    # Create test dataset using the remaining data\n",
        "    test_df = df.drop(initial_train_df.index).reset_index(drop=True)\n",
        "    test_descriptors = test_df[descriptor_columns].to_dict(\"records\")\n",
        "    test_targets = test_df[target_columns].to_dict(\"records\")\n",
        "    test_dataset = MoleculeDataset(test_descriptors, test_targets)\n",
        "    \n",
        "    # Determine model dimensions\n",
        "    input_dim = len(descriptor_columns)\n",
        "    output_dim = len(target_columns)\n",
        "    \n",
        "    # Train or continue training the model using the current labeled dataset\n",
        "    model = train_model(\n",
        "        dataset=train_dataset,\n",
        "        input_dim=input_dim,\n",
        "        output_dim=output_dim,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        hidden_dim=hidden_dim,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    \n",
        "    # Evaluate the model on the test set\n",
        "    test_loss, per_target_metrics = evaluate_model(model, test_dataset, batch_size=batch_size)\n",
        "    logger.info(f\"Iteration {iteration + 1} - Test Loss: {test_loss:.4f}\")\n",
        "    for m in per_target_metrics:\n",
        "        i = m[\"target_index\"]\n",
        "        logger.info(f\"[Target {i}] MSE={m['MSE']:.4f}, MAE={m['MAE']:.4f}, R2={m['R2_Score']:.4f}\")\n",
        "    \n",
        "    # If there is no more unlabeled data, stop the loop\n",
        "    if unlabeled_df.empty:\n",
        "        logger.info(\"No more unlabeled samples. Stopping active learning.\")\n",
        "        break\n",
        "    \n",
        "    # Use Monte Carlo Dropout to predict on unlabeled data and estimate uncertainty\n",
        "    unlabeled_descriptors = unlabeled_df[descriptor_columns].to_dict(\"records\")\n",
        "    unlabeled_dataset = MoleculeDataset(unlabeled_descriptors, targets=None)\n",
        "    predictions, uncertainties = predict_with_uncertainty(model, unlabeled_dataset, batch_size=batch_size)\n",
        "    \n",
        "    # Select the samples with the highest uncertainty\n",
        "    current_query_size = min(query_size, len(unlabeled_df))\n",
        "    query_indices = select_most_uncertain_samples(uncertainties, current_query_size)\n",
        "    \n",
        "    # Add the queried samples to the labeled dataset\n",
        "    queried_samples = unlabeled_df.iloc[query_indices]\n",
        "    initial_train_df = pd.concat([initial_train_df, queried_samples], ignore_index=True)\n",
        "    \n",
        "    # Remove the queried samples from the unlabeled dataset\n",
        "    unlabeled_df = unlabeled_df.drop(queried_samples.index).reset_index(drop=True)\n",
        "    logger.info(f\"Iteration {iteration + 1}: Queried {len(queried_samples)} samples.\")\n",
        "\n",
        "# Save the final active learning model\n",
        "output_dir = os.path.dirname(model_output)\n",
        "if output_dir and not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "    logger.info(f\"Created directory {output_dir}.\")\n",
        "torch.save(model, model_output)\n",
        "logger.info(f\"Active Learning completed. Final model saved to {model_output}\")\n",
        "\n",
        "# Save the final active learning training dataset\n",
        "initial_train_df.to_csv(final_al_training_data, index=False)\n",
        "logger.info(f\"Final AL training data saved to {final_al_training_data}\")\n",
        "```\n",
        "\n",
        "**Bash Command:**\n",
        "```bash\n",
        "# Run active learning workflow\n",
        "pyopac-active-learning \\\n",
        "  --initial-descriptors-file dataset/descriptors.csv \\\n",
        "  --initial-targets-file dataset/targets.csv \\\n",
        "  --unlabeled-descriptors-file dataset/unlabeled_descriptors.csv \\\n",
        "  --unlabeled-targets-file dataset/unlabeled_targets.csv \\\n",
        "  --budget 10\n",
        "```\n",
        "\n",
        "### 10.9 Step 8: Predict with Active Learning Model\n",
        "\n",
        "**Python Code:**\n",
        "```python\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from opac.data.dataset import MoleculeDataset\n",
        "from opac.utils.logger import get_logger\n",
        "\n",
        "logger = get_logger(__name__)\n",
        "\n",
        "# Set the paths for the model, the input descriptors, and where to save predictions\n",
        "model_file = os.path.join(\"saved_models\", \"al_trained_model.pth\")\n",
        "descriptors_file = os.path.join(\"dataset\", \"new_descriptors.csv\")\n",
        "predictions_output = os.path.join(\"dataset\", \"new_predictions.csv\")\n",
        "\n",
        "# The CSV should contain a column `mol_id` plus descriptor columns\n",
        "df_descriptors = pd.read_csv(descriptors_file)\n",
        "# Assume the descriptor columns are all columns except 'mol_id'\n",
        "descriptor_columns = [col for col in df_descriptors.columns if col != \"mol_id\"]\n",
        "descriptors = df_descriptors[descriptor_columns].to_dict(\"records\")\n",
        "print(f\"Loaded descriptors for {len(descriptors)} molecules.\")\n",
        "\n",
        "# We create a dataset from the descriptors. Since we're only predicting, no targets are needed\n",
        "dataset = MoleculeDataset(descriptors, targets=None)\n",
        "\n",
        "# The model was saved using `torch.save(model, model_file)`, so we load the entire model\n",
        "model = torch.load(model_file)\n",
        "model.eval()\n",
        "print(f\"Loaded active learning model from {model_file}\")\n",
        "\n",
        "# For each sample in the dataset, we add a batch dimension to the descriptor tensor,\n",
        "# run the model, and collect the predictions\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for sample in dataset:\n",
        "        # Add batch dimension to the descriptor tensor\n",
        "        inputs = sample[\"descriptors\"].unsqueeze(0)\n",
        "        outputs = model(inputs)\n",
        "        predictions.append(outputs.numpy().flatten())\n",
        "\n",
        "# Create a DataFrame with the predictions and the corresponding `mol_id`, then save to CSV\n",
        "if predictions:\n",
        "    output_dim = predictions[0].shape[0]\n",
        "    property_names = [f\"Predicted_Property_{i+1}\" for i in range(output_dim)]\n",
        "else:\n",
        "    property_names = []\n",
        "\n",
        "df_preds = pd.DataFrame(predictions, columns=property_names)\n",
        "df_preds[\"mol_id\"] = df_descriptors[\"mol_id\"]\n",
        "\n",
        "df_preds.to_csv(predictions_output, index=False)\n",
        "logger.info(f\"Saved predictions to {predictions_output}\")\n",
        "print(f\"Predictions saved to {predictions_output}\")\n",
        "```\n",
        "\n",
        "**Bash Command:**\n",
        "```bash\n",
        "# Predict with active learning model (same as regular prediction)\n",
        "pyopac-predict \\\n",
        "  --model-file saved_models/al_trained_model.pth \\\n",
        "  --descriptors-file dataset/new_descriptors.csv \\\n",
        "  --predictions-output dataset/al_predictions.csv\n",
        "```\n",
        "\n",
        "### 10.10 Additional Command-Line Tools\n",
        "\n",
        "**Generate Molecules (VAE):**\n",
        "```bash\n",
        "# Train VAE and generate new molecule descriptors\n",
        "pyopac-generate \\\n",
        "  --descriptors-file dataset/descriptors.csv \\\n",
        "  --vae-model-output saved_models/vae_model.pth \\\n",
        "  --generated-descriptors-output dataset/generated_descriptors.csv \\\n",
        "  --num-samples 100 \\\n",
        "  --epochs 100\n",
        "```\n",
        "\n",
        "**Get Help for Any Command:**\n",
        "```bash\n",
        "# Get help for any command-line tool\n",
        "pyopac-modify-xyz --help\n",
        "pyopac-preprocess --help\n",
        "pyopac-compute-descriptors --help\n",
        "pyopac-train --help\n",
        "pyopac-predict --help\n",
        "pyopac-generate --help\n",
        "pyopac-active-learning --help\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Key Features Summary\n",
        "\n",
        "### SOAP Descriptors for Gas/Solution Phase Energy Prediction\n",
        "\n",
        "- **Size-Invariant**: All molecules produce descriptors with the same fixed size (works for any molecule size)\n",
        "- **Rotationally Equivariant**: Descriptors transform predictably with molecular rotations\n",
        "- **Translationally Invariant**: Same descriptor regardless of molecular position\n",
        "- **3D Structure-Aware**: Based on actual atomic positions, capturing molecular geometry\n",
        "\n",
        "### Important Notes for This Example\n",
        "\n",
        "1. **Fixed Species List**: Always use the same species list (`all_species`) for all molecules (training and test) to ensure consistent descriptor sizes. This is saved in `species_list.json`.\n",
        "\n",
        "2. **Descriptor Parameters**:\n",
        "   - `nmax=6`: Number of radial basis functions (controls radial resolution)\n",
        "   - `lmax=4`: Maximum angular momentum (higher values better for angular details)\n",
        "   - `rcut=6.0`: Cutoff radius in Angstrom (neighbors within this distance)\n",
        "   - `sigma=0.3`: Gaussian smearing width (controls locality)\n",
        "\n",
        "3. **Multi-Target Prediction**: The model predicts multiple properties simultaneously:\n",
        "   - Gas_Phase_Energy (scalar) - Energy in gas phase\n",
        "   - Solution_Phase_Energy (scalar) - Energy in solution phase\n",
        "   - Solvation_Free_Energy (scalar) - Free energy of solvation (calculated as difference)\n",
        "\n",
        "4. **Data Format**: The `results_train.dat` file contains:\n",
        "   - Column 1: Gas_Phase_Energy (eV)\n",
        "   - Column 2: Solution_Phase_Energy (eV)\n",
        "   - Solvation_Free_Energy is calculated as the difference\n",
        "\n",
        "5. **Size-Invariance**: By averaging SOAP descriptors over all atoms, we create a global, fixed-size representation regardless of molecule size. This is critical for handling molecules of different sizes in the same dataset.\n",
        "\n",
        "## Real-World Application\n",
        "\n",
        "This example demonstrates predicting:\n",
        "- **Gas phase energies** from DFT calculations (PySCF)\n",
        "- **Solution phase energies** from DFT calculations with solvent model (ddCOSMO)\n",
        "- **Solvation free energies** (calculated from the difference)\n",
        "\n",
        "See `examples/1_gas_phase_solution_phase_energy/dft_calculation.py` for how to compute these properties using PySCF. The `results_train.dat` file contains the actual DFT calculation results.\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- See `QUICK_START.md` for more examples\n",
        "- Check `README.md` for complete documentation\n",
        "- Try the `examples/1_gas_phase_solution_phase_energy/` directory for the actual dataset\n",
        "- Use `examples/TEST_TODAY/` for a complete pipeline example\n",
        "\n",
        "## Support\n",
        "\n",
        "For issues or questions:\n",
        "- Check the documentation files\n",
        "- See `examples/1_gas_phase_solution_phase_energy/` for the actual data structure\n",
        "- Contact: eosaro@nd.edu, ycolon@nd.edu"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
