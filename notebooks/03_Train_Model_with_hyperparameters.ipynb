{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "\n",
    "# Import our unified dataset and training functions\n",
    "from opac.data.dataset import MoleculeDataset\n",
    "from opac.models.trainer import train_model, evaluate_model\n",
    "\n",
    "# Load descriptors and targets\n",
    "df_descriptors = pd.read_csv(os.path.join(\"dataset\", \"descriptors.csv\"))\n",
    "df_targets = pd.read_csv(os.path.join(\"dataset\", \"targets.csv\"))\n",
    "\n",
    "# Ensure 'mol_id' is numeric and merge on 'mol_id'\n",
    "df_descriptors['mol_id'] = df_descriptors['mol_id'].astype(int)\n",
    "df_targets['mol_id'] = df_targets['mol_id'].astype(int)\n",
    "df = pd.merge(df_descriptors, df_targets, on='mol_id')\n",
    "\n",
    "# Identify descriptor and target columns\n",
    "descriptor_cols = [col for col in df_descriptors.columns if col != 'mol_id']\n",
    "target_cols = [col for col in df_targets.columns if col != 'mol_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_descriptors = train_df[descriptor_cols].to_dict('records')\n",
    "train_targets = train_df[target_cols].to_dict('records')\n",
    "test_descriptors = test_df[descriptor_cols].to_dict('records')\n",
    "test_targets = test_df[target_cols].to_dict('records')\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MoleculeDataset(train_descriptors, train_targets)\n",
    "test_dataset  = MoleculeDataset(test_descriptors, test_targets)\n",
    "\n",
    "# Get dimensions for model input/output\n",
    "input_dim = train_dataset.input_dim\n",
    "output_dim = train_dataset.output_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.00, 0.0001],\n",
    "    'hidden_dim': [128, 256],\n",
    "    'weight_decay': [0.0, 1e-4]\n",
    "}\n",
    "\n",
    "# Hyperparameter tuning via grid search\n",
    "best_loss = float('inf')\n",
    "best_params = None\n",
    "best_model_state = None\n",
    "\n",
    "# Create combinations of hyperparameters\n",
    "for lr, hidden, wd in itertools.product(param_grid['learning_rate'],\n",
    "                                          param_grid['hidden_dim'],\n",
    "                                          param_grid['weight_decay']):\n",
    "    print(f\"Training with lr={lr}, hidden_dim={hidden}, weight_decay={wd}\")\n",
    "\n",
    "    # Train the model using the current hyperparameters.\n",
    "    # We pass 'existing_model=None' to train from scratch each time.\n",
    "    model = train_model(\n",
    "        dataset=train_dataset,\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        epochs=200,              # You can adjust the number of epochs\n",
    "        batch_size=64,\n",
    "        learning_rate=lr,\n",
    "        hidden_dim=hidden,\n",
    "        weight_decay=wd\n",
    "    )\n",
    "    # Evaluate the model on the validation set.\n",
    "    test_loss, _ = evaluate_model(model, test_dataset, batch_size=32)\n",
    "    print(f\"Validation loss: {test_loss:.4f}\")\n",
    "\n",
    "    # If current configuration is better, store its parameters and model state.\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        best_params = {'input_dim': input_dim, 'output_dim': output_dim,'learning_rate': lr, 'hidden_dim': hidden, 'weight_decay': wd}\n",
    "        best_model_state = model.state_dict()  # Save the state_dict\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_params)\n",
    "print(f\"Best Validation Loss: {best_loss:.4f}\")\n",
    "\n",
    "torch.save(best_model_state, \"saved_models/best_trained_model.pth\")\n",
    "with open(\"saved_models/best_trained_model.pth.params.json\", \"w\") as f:\n",
    "    json.dump(best_params, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
